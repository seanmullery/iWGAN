{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you to the following for the code on which this program is based.\n",
    "\n",
    "For iWGAN implementation\n",
    "https://github.com/Shaofanl/Keras-GAN/blob/master/GAN/models/GAN.py\n",
    "\n",
    "For Layernorm implementation\n",
    "https://github.com/DingKe/nn_playground/blob/master/layernorm/layer_norm_layers.py\n",
    "\n",
    "To Francois Chollet for his Keras framework and book Deep Learing with Python.\n",
    "Pages 308-311 give some basic GAN code to get started with.\n",
    "\n",
    "The paper by Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville which introduced the iWGAN is here. \n",
    "https://arxiv.org/abs/1704.00028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.lines as mlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras import layers\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Input\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "from keras.constraints import max_norm\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import inspect\n",
    "import contextlib\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm import trange\n",
    "import pydot\n",
    "import graphviz\n",
    "from keras.utils import plot_model\n",
    "import os\n",
    "from keras.preprocessing import image\n",
    "import csv\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras import initializers, regularizers\n",
    "from keras.utils.generic_utils import serialize_keras_object\n",
    "from keras.utils.generic_utils import deserialize_keras_object\n",
    "from keras.legacy import interfaces\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "#base path where you wish to save output files\n",
    "save_dir = './'\n",
    "\n",
    "#save hyper parameter details to a txt file.\n",
    "f = open(os.path.join(save_dir, 'out/model/hyperparameters.txt'),'a')\n",
    "f.write('\\n' + 'latent_dim = ' + str(latent_dim))\n",
    "f.write('\\n' + 'height = ' + str(height))\n",
    "f.write('\\n' + 'width = ' + str(width))\n",
    "f.write('\\n' + 'channels = ' + str(channels))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Layer normiliziation implementation from the following source.\n",
    "### https://github.com/DingKe/nn_playground/blob/master/layernorm/layer_norm_layers.py\n",
    "### this is used in the discriminator architecture.\n",
    "\n",
    "def to_list(x):\n",
    "    if type(x) not in [list, tuple]:\n",
    "        return [x]\n",
    "    else:\n",
    "        return list(x)\n",
    "\n",
    "def LN(x, gamma, beta, epsilon=1e-6, axis=-1):\n",
    "    m = K.mean(x, axis=axis, keepdims=True)\n",
    "    std = K.sqrt(K.var(x, axis=axis, keepdims=True) + epsilon)\n",
    "    x_normed = (x - m) / (std + epsilon)\n",
    "    x_normed = gamma * x_normed + beta\n",
    "\n",
    "    return x_normed\n",
    "\n",
    "\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, axis=-1,\n",
    "                 gamma_init='one', beta_init='zero',\n",
    "                 gamma_regularizer=None, beta_regularizer=None,\n",
    "                 epsilon=1e-6, **kwargs): \n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "\n",
    "        self.axis = to_list(axis)\n",
    "        self.gamma_init = initializers.get(gamma_init)\n",
    "        self.beta_init = initializers.get(beta_init)\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = [1 for _ in input_shape]\n",
    "        for i in self.axis:\n",
    "            shape[i] = input_shape[i]\n",
    "        self.gamma = self.add_weight(shape=shape,\n",
    "                                     initializer=self.gamma_init,\n",
    "                                     regularizer=self.gamma_regularizer,\n",
    "                                     name='gamma')\n",
    "        self.beta = self.add_weight(shape=shape,\n",
    "                                    initializer=self.beta_init,\n",
    "                                    regularizer=self.beta_regularizer,\n",
    "                                    name='beta')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        return LN(inputs, gamma=self.gamma, beta=self.beta, \n",
    "                  axis=self.axis, epsilon=self.epsilon)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'epsilon': self.epsilon,\n",
    "                  'axis': self.axis,\n",
    "                  'gamma_init': initializers.serialize(self.gamma_init),\n",
    "                  'beta_init': initializers.serialize(self.beta_init),\n",
    "                  'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
    "                  'beta_regularizer': regularizers.serialize(self.gamma_regularizer)}\n",
    "        base_config = super(LayerNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator_input =  keras.Input(shape=(latent_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Basic clipping layer which can be used in conjunction with Lambda layer in output of Generator.\n",
    "def clipLayer(x):\n",
    "    return keras.backend.clip(x, -1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Setup initializer values for keras RandomNormal which will be used in initialization of weights.\n",
    "mean = 0.0\n",
    "stddev = 0.02\n",
    "keras.initializers.RandomNormal(mean=mean, stddev=stddev, seed=None)\n",
    "f = open(os.path.join(save_dir, 'out/model/hyperparameters.txt'),'a')\n",
    "f.write('\\n' + 'initialisers mean = ' + str(mean))\n",
    "f.write('\\n' + 'initialisers stddev = ' + str(stddev))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First, Transform the input into a 4 X 4 x 1024 channels feature map\n",
    "x = layers.Dense(1024 * 4 * 4, kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros')(generator_input)\n",
    "x = Activation('relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "x = layers.Reshape((4, 4, 1024))(x)\n",
    "\n",
    "\n",
    "#Then add a convolution layer\n",
    "x = layers.Conv2D(1024, 5, padding='same',  kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "\n",
    "#Upsample to 8 x 8 reduce number of filters\n",
    "x = layers.UpSampling2D()(x)\n",
    "x = layers.Conv2D(512, 3, padding='same',  kernel_initializer='random_normal',\n",
    "           bias_initializer='zeros')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "#Upsample to 16 x 16 reduce number of filters\n",
    "x = layers.UpSampling2D()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same',  kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "#Upsample to 32 x 32 reduce number of filters\n",
    "x = layers.UpSampling2D()(x)\n",
    "x = layers.Conv2D(128, 5, padding='same',  kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "#Produce a 32 x 32 l-channel feature map\n",
    "x = layers.Conv2D(channels, 7, padding='same', kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros')(x)\n",
    "\n",
    "### Decide whether to use tanh-alone, BN-with-tanh or BN-with-clipping\n",
    "### by uncommenting the appropriate lines below.\n",
    "#x = layers.BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)   \n",
    "#x=layers.Lambda(clipLayer, tuple(list((32,32,3))))(x)\n",
    "generator = keras.models.Model(generator_input, x)\n",
    "\n",
    "#print a plot of the model to the output directory.\n",
    "plot_model(generator, show_shapes=True, to_file=os.path.join(save_dir, 'out/model/generator.png'))\n",
    "\n",
    "# Show a summary of the Generator network by uncommenting the next line.\n",
    "#generator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the weights of the final BN Layer\n",
    "If you wish to set the initial weights of the final BatchNorm layer you can do so by uncommenting the line below.\n",
    "I have initialized to the RGB standard-deviation and RGB mean of the target distribution.\n",
    "final two lists are the running-mean and running-SD which we set to 0 and 1 respectively\n",
    "Keras BatchNorm implementation is here for reference\n",
    "https://github.com/keras-team/keras/blob/master/keras/layers/normalization.py\n",
    "if you change the architecture you will have to determine the correct layer number \n",
    "by using generator.summary() you can count your way down to the appropriate BN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#generator.layers[21].set_weights(np.array([[0.45776676431247115, 0.4371233749533978, 0.440839876713472],[-0.0598859, -0.123213, -0.309562], [0,0,0], [1,1,1]]))\n",
    "\n",
    "### you may wish to freeze the weights of the final BN Layer. If so you can do that with the following line.\n",
    "#generator.layers[21].trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### The iWGAN enforces the lipschitz contstraint with a soft penalty on the norm of the gradient.\n",
    "class GradNorm(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GradNorm, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        super(GradNorm, self).build(input_shapes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        target, wrt = inputs\n",
    "        grads = K.gradients(target, wrt)\n",
    "        assert len(grads) == 1\n",
    "        grad = grads[0]\n",
    "        \n",
    "        return K.sqrt(K.sum(K.batch_flatten(K.square(grad)), axis=1, keepdims=True))\n",
    "    \n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        return (input_shapes[1][0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(pred, label):    \n",
    "    return K.mean(label*pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "x = layers.Conv2D(128, 3, kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros' )(discriminator_input)\n",
    "#\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = LayerNormalization()(x)\n",
    "\n",
    "x = layers.Conv2D(256, 4, strides=2, kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros' )(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = LayerNormalization()(x)#x = Activation('tanh')(x)\n",
    "\n",
    "x = layers.Conv2D(512, 4, strides=2, kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros')(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = LayerNormalization()(x)\n",
    "\n",
    "x = layers.Conv2D(1024, 4, strides=2, kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros')(x)\n",
    "x = layers.LeakyReLU(0.2)(x)\n",
    "x = LayerNormalization()(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dense(1, kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros')(x)\n",
    "\n",
    "discriminator = keras.models.Model(discriminator_input, x)\n",
    "plot_model(discriminator,show_shapes=True, to_file=os.path.join(save_dir, 'out/model/discriminator.png'))\n",
    "#discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discriminator_optimizer = keras.optimizers.Adam(lr=0.0001,beta_1=0.9, beta_2=0.99)\n",
    "discriminator.compile(optimizer=discriminator_optimizer, loss = discriminator_loss)\n",
    "\n",
    "### update hyperparamters file.\n",
    "f = open(os.path.join(save_dir, 'out/model/hyperparameters.txt'),'a')\n",
    "f.write('\\n' + 'discriminator optimizer is Adam(lr=0.0001,beta_1=0.9, beta_2=0.99) ' )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The adversarial network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gan_loss(pred, label):\n",
    "    return K.mean(label*pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_loss(pred, label):\n",
    "    return K.mean(pred*label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set discriminator weights to non-trainable\n",
    "# (will only apply to the 'gan' model)\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input)) \n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "\n",
    "gan_optimizer = keras.optimizers.Adam(lr=0.00001,beta_1=0.9, beta_2=0.99)\n",
    "gan.compile(optimizer=gan_optimizer, loss=gan_loss)\n",
    "\n",
    "#Record Hyperparameters\n",
    "f = open(os.path.join(save_dir, 'out/model/hyperparameters.txt'),'a')\n",
    "f.write('\\n' + 'GAN optimizer is Adam(lr=0.00001,beta_1=0.9, beta_2=0.99)' )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.merge import _Merge\n",
    "\n",
    "class Subtract(_Merge):\n",
    "    def _merge_function(self, inputs):\n",
    "        output = inputs[0]\n",
    "        for i in range(1, len(inputs)):\n",
    "            output = output-inputs[i]\n",
    "            #output = input\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.merge import _Merge\n",
    "\n",
    "class Square(_Merge):\n",
    "    def _merge_function(self, input):\n",
    "        output = input*input\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmbd = 10\n",
    "shape = discriminator.get_input_shape_at(0)[1:]\n",
    "gen_input, real_input, interpolation = keras.Input(shape), keras.Input(shape), keras.Input(shape)\n",
    "\n",
    "sub = Subtract()([discriminator(gen_input), discriminator(real_input)])\n",
    "norm = GradNorm()([discriminator(interpolation), interpolation])\n",
    "\n",
    "val = keras.Input(norm.get_shape()[1:])\n",
    "normal = Subtract()([norm, val])\n",
    "normal_sq = keras.layers.multiply([normal, normal])\n",
    "\n",
    "dis2batch = keras.models.Model([real_input, gen_input, interpolation, val], [sub, normal_sq]) \n",
    "discriminator.trainable = True\n",
    "dis2batch.compile(optimizer=discriminator_optimizer, loss=[mean_loss,'mse'], loss_weights=[1.0, lmbd])\n",
    "\n",
    "#Create plot of dis2batch model in output directory\n",
    "plot_model(dis2batch,show_shapes=True, to_file=os.path.join(save_dir, 'out/model/dis2batch.png'))\n",
    "#dis2batch.summary()\n",
    "f = open(os.path.join(save_dir, 'out/model/hyperparameters.txt'),'a')\n",
    "f.write('\\n' + 'loss weights are [1.0, lmbd] ' )\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load CIFAR10 data\n",
    "(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "#select frog images(class 6)\n",
    "x_train = x_train[y_train.flatten() == 6]\n",
    "\n",
    "# Normalize data to range [-1,1]\n",
    "x_train = ((x_train.reshape((x_train.shape[0],) + (height, width, channels)).astype('float32'))/127.5 - 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### This function can be called if you want to see a png of the critic loss\n",
    "### I don't use this below but it is useful to output periodically during trainging \n",
    "### as opening the csv file can lead to file corruption.\n",
    "### Be wary tough, it is a slow operation if there are a lot of datapoints so it can\n",
    "### slow everything down lote in\n",
    "def outputCriticPlot():        \n",
    "    plt.plot(d_loss_short_curve,label='Discriminator Loss')    \n",
    "    plt.title('Critic loss Curves')   \n",
    "    plt.savefig(os.path.join(save_dir, 'out/model/Critic_Loss.png'))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "d_loss_curve = []\n",
    "# Start training loop\n",
    "start = 0\n",
    "current_step = 0\n",
    "gen_count = 0 #Keep a count of how many iterations of the generator we have\n",
    "dis_count = 0 #keep a count of how many iterations of the discriminator we have\n",
    "iterations = 30001\n",
    "t =  trange(iterations, desc='')\n",
    "initial_critic_schedule = 50\n",
    "normal_critic_schedule = 5\n",
    "f = open(os.path.join(save_dir, 'out/model/hyperparameters.txt'),'a')\n",
    "f.write('\\n' + 'batch_size = ' + str(batch_size))\n",
    "f.write('\\n' + 'intital_critic_schedule = ' + str(initial_critic_schedule) +\"first 25 then every 500\")\n",
    "f.write('\\n' + 'normal_critic_schedule = ' + str(normal_critic_schedule))\n",
    "\n",
    "f.close()\n",
    "\n",
    "a_loss = 0\n",
    "d_loss = 0\n",
    "generated_images=0\n",
    "interpolation=0\n",
    "value=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for step in t:\n",
    "    \n",
    "    if current_step < 25 or current_step%500 ==0: \n",
    "        n_critic = initial_critic_schedule\n",
    "       \n",
    "    else:\n",
    "        n_critic = normal_critic_schedule\n",
    "       \n",
    "   \n",
    "    ##################  Critic/Discriminator Loop  ##############################################\n",
    "    \n",
    "    for dis_step in range(n_critic):\n",
    "        # I don't think the following line is necessary along with it's counterpart below \n",
    "        discriminator.trainable = True\n",
    "        dis_count = dis_count + 1\n",
    "        \n",
    "        #Sample random points in the latent space\n",
    "        random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    \n",
    "        # Decode them to fake images\n",
    "        generated_images = generator.predict(random_latent_vectors)\n",
    "                \n",
    "        #combine them with real images\n",
    "        stop = start + batch_size\n",
    "        real_images = x_train[start: stop]\n",
    "        epsilon = np.random.uniform(0, 1, size=(batch_size,1,1,1))\n",
    "        interpolation = epsilon*real_images + (1-epsilon)*generated_images\n",
    "        value = np.ones((batch_size, 1))\n",
    "    \n",
    "        # Assemble lables discriminating real from fake images\n",
    "        labelsFake = np.ones((batch_size, 1))\n",
    "        labelsReal = -np.ones((batch_size, 1))\n",
    "                   \n",
    "        #Train the discriminator\n",
    "        \n",
    "        d_loss, d_diff, d_norm = dis2batch.train_on_batch([real_images, generated_images, interpolation, value], \n",
    "                                                          [np.ones((batch_size, 1))]*2)\n",
    "        \n",
    "        start += batch_size\n",
    "        if start > len(x_train) - batch_size:\n",
    "            start = 0   \n",
    "   \n",
    "    \n",
    "    ###################### Generator Single Iteration #########################################################\n",
    "  \n",
    "    gen_count = gen_count + 1\n",
    "    # I don't think the following line is necessary along with it's counterpart above \n",
    "    discriminator.trainable = False\n",
    "    # samples random points in the latent space\n",
    "    \n",
    "    random_latent_vectors = np.random.normal(size= (batch_size, latent_dim))\n",
    "    \n",
    "    #Assemble labels that say \"all real images\"\n",
    "    misleading_targets = -np.ones((batch_size, 1))\n",
    "    \n",
    "    # Train the generator (via the gan model,\n",
    "    # where the discrimator weights are frozen)        \n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "     \n",
    "    ##################################################################################################\n",
    "    \n",
    "    d_loss_curve.append(d_loss)\n",
    "       \n",
    "    if(current_step%50==0):\n",
    "                               \n",
    "        #Save one generated image\n",
    "        img = image.array_to_img((generated_images[0]+1) * 127.5, scale=False)\n",
    "        img.save(os.path.join(save_dir, 'out/gen/generated_frog' + str(current_step)+'.png'))\n",
    "               \n",
    "        with open(os.path.join(save_dir, 'out/model/critic_loss.csv'), 'w') as myfile:\n",
    "            wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "            wr.writerow(d_loss_curve)\n",
    "       \n",
    "    if current_step % 5000 == 0:\n",
    "        gan.save_weights(os.path.join(save_dir, 'out/model/' + str(current_step) + 'gan.h5'))\n",
    "         \n",
    "    current_step = current_step + 1\n",
    "    \n",
    "f = open(os.path.join(save_dir, 'out/model/hyperparameters.txt'),'a')\n",
    "f.write('\\n' + 'Critic iterations = ' + str(dis_count))\n",
    "f.close()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
